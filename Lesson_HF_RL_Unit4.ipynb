{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOARhkzijlkBrw2TCh7Tp6T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Art-phys/Lesson_HF_LR/blob/main/Lesson_HF_RL_Unit4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ë–ª–æ–∫ 4: –°–æ–∑–¥–∞–π—Ç–µ —Å–≤–æ–π –ø–µ—Ä–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –≥–ª—É–±–æ–∫–∏–º –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø–æ–º–æ—â—å—é PyTorch: Reinforce. –ò –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –µ–≥–æ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å üí™\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png\" alt=\"thumbnail\" width=\"50%\"/>\n",
        "\n",
        "–í —ç—Ç–æ–º –±–ª–æ–∫–Ω–æ—Ç–µ –≤—ã —Å –Ω—É–ª—è —Å–æ–∑–¥–∞–¥–∏—Ç–µ —Å–≤–æ–π –ø–µ—Ä–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –≥–ª—É–±–æ–∫–∏–º –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º: Reinforcement (—Ç–∞–∫–∂–µ –Ω–∞–∑—ã–≤–∞–µ–º—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–º –ø–æ–ª–∏—Ç–∏–∫–∏ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ).\n",
        "\n",
        "–£—Å–∏–ª–µ–Ω–∏–µ (Reinforce) - —ç—Ç–æ *–º–µ—Ç–æ–¥, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø–æ–ª–∏—Ç–∏–∫–µ*: –∞–ª–≥–æ—Ä–∏—Ç–º –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –ø—ã—Ç–∞–µ—Ç—Å—è **–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–∏—Ç–∏–∫—É –Ω–∞–ø—Ä—è–º—É—é, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—è —Ñ—É–Ω–∫—Ü–∏—é –∑–Ω–∞—á–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—è**.\n",
        "\n",
        "–¢–æ—á–Ω–µ–µ, –£—Å–∏–ª–µ–Ω–∏–µ (Reinforce) - —ç—Ç–æ *–ú–µ—Ç–æ–¥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ–ª–∏—Ç–∏–∫–∏*, –ø–æ–¥–∫–ª–∞—Å—Å *–º–µ—Ç–æ–¥–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –ø–æ–ª–∏—Ç–∏–∫–µ*, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω **–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –ø—É—Ç–µ–º –æ—Ü–µ–Ω–∫–∏ –≤–µ—Å–æ–≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–¥—ä–µ–º–∞**.\n",
        "\n",
        "–ß—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –µ–≥–æ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å, –º—ã —Å–æ–±–∏—Ä–∞–µ–º—Å—è –æ–±—É—á–∏—Ç—å –µ–≥–æ –≤ –¥–≤—É—Ö —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ—Å—Ç—ã—Ö —Å—Ä–µ–¥–∞—Ö:\n",
        "- Cartpole-v1 (–°—Ç–æ–ª–± —Ç–µ–ª–µ–∂–∫–∏-v1)\n",
        "- PixelcopterEnv (–ü–∏–∫—Å–µ–ª—å–Ω—ã–π –≤–µ—Ä—Ç–æ–ª–µ—Ç)\n",
        "\n",
        "‚¨áÔ∏è –í–æ—Ç –ø—Ä–∏–º–µ—Ä —Ç–æ–≥–æ, —á–µ–≥–æ **–≤—ã –¥–æ—Å—Ç–∏–≥–Ω–µ—Ç–µ –≤ –∫–æ–Ω—Ü–µ —ç—Ç–æ–º –±–ª–æ–∫–Ω–æ—Ç–µ.** ‚¨áÔ∏è"
      ],
      "metadata": {
        "id": "uDL2CAkWTjx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif\" alt=\"Environments\" width=\"50%\"/>\n"
      ],
      "metadata": {
        "id": "wqFm3-_dVhrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéÆ –û–∫—Ä—É–∂–∞—é—â–∞—è —Å—Ä–µ–¥–∞: \n",
        "\n",
        "- [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n",
        "- [PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n",
        "\n",
        "### üìö RL-–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞: \n",
        "\n",
        "- Python\n",
        "- PyTorch\n",
        "\n",
        "\n",
        " [GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
      ],
      "metadata": {
        "id": "GptTjtpKban6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –¶–µ–ª–∏ —ç—Ç–æ–≥–æ –±–ª–æ–∫–Ω–æ—Ç–∞ üèÜ\n",
        "–í –∫–æ–Ω—Ü–µ –≤—ã –±—É–¥–µ—Ç–µ:\n",
        "- –£–º–µ—Ç—å **—Å –Ω—É–ª—è –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º —É—Å–∏–ª–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PyTorch.**\n",
        "- –ò–º–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å **–ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –≤–∞—à–µ–≥–æ –∞–≥–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–æ—Å—Ç—ã–µ —Å—Ä–µ–¥—ã.**\n",
        "- –ò–º–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å **–æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–≤–æ–µ–≥–æ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –≤ —Ü–µ–Ω—Ç—Ä** —Å —Ö–æ—Ä–æ—à–∏–º –≤–∏–¥–µ–æ–ø–æ–≤—Ç–æ—Ä–æ–º –∏ –æ—Ü–µ–Ω–æ—á–Ω—ã–º –±–∞–ª–ª–æ–º üî• ."
      ],
      "metadata": {
        "id": "8XBD7_HTb3NK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –≠—Ç–∞ —Ç–µ—Ç—Ä–∞–¥—å –≤–∑—è—Ç–∞ –∏–∑ –∫—É—Ä—Å–∞ –æ–±—É—á–µ–Ω–∏—è —Å –≥–ª—É–±–æ–∫–∏–º –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\" width=\"50%\"/>"
      ],
      "metadata": {
        "id": "VU5d8p5qcQbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ù–∞ —ç—Ç–æ–º –±–µ—Å–ø–ª–∞—Ç–Ω–æ–º –∫—É—Ä—Å–µ –≤—ã:\n",
        "\n",
        "- üìñ –ò–∑—É—á–∏—Ç–µ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ **—Ç–µ–æ—Ä–∏–∏ –∏ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ**.\n",
        "- üßë üíª –ù–∞—É—á–∏—Ç–µ—Å—å **–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Deep RL**, —Ç–∞–∫–∏–µ –∫–∞–∫ Stable Baselines3, RL Baselines3 Zoo, CleanRL –∏ Sample Factory 2.0.\n",
        "- ü§ñ –û–±—É—á–∏—Ç–µ **–∞–≥–µ–Ω—Ç–æ–≤ –≤ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö**"
      ],
      "metadata": {
        "id": "g-sXRdpDcjxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ü—Ä–µ–¥–ø–æ—Å—ã–ª–∫–∏ üèóÔ∏è\n",
        "–ü—Ä–µ–∂–¥–µ —á–µ–º –ø–æ–≥—Ä—É–∑–∏—Ç—å—Å—è –≤ –±–ª–æ–∫–Ω–æ—Ç, –≤–∞–º –Ω—É–∂–Ω–æ:\n",
        "\n",
        "üî≤ üìö [–ò–∑—É—á–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ–ª–∏—Ç–∏–∫–∏, –ø—Ä–æ—á–∏—Ç–∞–≤ Unit 4](https://huggingface.co/deep-rl-course/unit4/introduction)"
      ],
      "metadata": {
        "id": "5oQ2cJ_mc58a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –î–∞–≤–∞–π—Ç–µ –∑–∞–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä—É–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º —É—Å–∏–ª–µ–Ω–∏—è —Å –Ω—É–ª—è üî•\n",
        "\n",
        "\n",
        "–ß—Ç–æ–±—ã –ø—Ä–æ–π—Ç–∏ —ç—Ç–æ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏, –≤–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–≤–æ–∏ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –¶–µ–Ω—Ç—Ä.\n",
        "\n",
        "- –ü–æ–ª—É—á–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç >= 350 –¥–ª—è `Cart pole-v1`.\n",
        "- –ü–æ–ª—É—á–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç >= 5 –¥–ª—è `Pixel Copter`.\n",
        "\n",
        "–ß—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å —Å–≤–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –ø–µ—Ä–µ–π–¥–∏—Ç–µ –≤ —Ç–∞–±–ª–∏—Ü—É –ª–∏–¥–µ—Ä–æ–≤ –∏ –Ω–∞–π–¥–∏—Ç–µ —Å–≤–æ—é –º–æ–¥–µ–ª—å, **the result = mean_reward - std_reward**. **–ï—Å–ª–∏ –≤—ã –Ω–µ –≤–∏–¥–∏—Ç–µ —Å–≤–æ—é –º–æ–¥–µ–ª—å –≤ —Ç–∞–±–ª–∏—Ü–µ –ª–∏–¥–µ—Ä–æ–≤, –ø–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –Ω–∏–∂–Ω—é—é —á–∞—Å—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–∞–±–ª–∏—Ü—ã –ª–∏–¥–µ—Ä–æ–≤ –∏ –Ω–∞–∂–º–∏—Ç–µ –Ω–∞ –∫–Ω–æ–ø–∫—É –æ–±–Ω–æ–≤–∏—Ç—å**.\n",
        "\n",
        "–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ—Ü–µ—Å—Å–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ–∑–Ω–∞–∫–æ–º—å—Ç–µ—Å—å —Å —ç—Ç–∏–º —Ä–∞–∑–¥–µ–ª–æ–º üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
      ],
      "metadata": {
        "id": "PKdxmqgNdSpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –°–æ–≤–µ—Ç üí°\n",
        "–õ—É—á—à–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å —ç—Ç–æ—Ç colab –≤ –∫–æ–ø–∏–∏ –Ω–∞ –≤–∞—à–µ–º Google –î–∏—Å–∫–µ, —á—Ç–æ–±—ã **–≤ —Å–ª—É—á–∞–µ –∏—Å—Ç–µ—á–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–∂–∏–¥–∞–Ω–∏—è** —É –≤–∞—Å –≤—Å–µ –µ—â–µ –±—ã–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –∑–∞–ø–∏—Å–Ω–∞—è –∫–Ω–∏–∂–∫–∞ –Ω–∞ –≤–∞—à–µ–º Google –î–∏—Å–∫–µ, –∏ –≤–∞–º –Ω–µ –Ω—É–∂–Ω–æ –±—ã–ª–æ –∑–∞–ø–æ–ª–Ω—è—Ç—å –≤—Å–µ —Å –Ω—É–ª—è.\n",
        "\n",
        "–î–ª—è —ç—Ç–æ–≥–æ –≤—ã –º–æ–∂–µ—Ç–µ –ª–∏–±–æ –Ω–∞–∂–∞—Ç—å `Ctrl + S` –ª–∏–±–æ `File > Save a copy in Google Drive.`"
      ],
      "metadata": {
        "id": "Xp28ZSL7d9oL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ó–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∞ GPU üí™\n",
        "- –ß—Ç–æ–±—ã **—É—Å–∫–æ—Ä–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä GPU. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ `Runtime > Change Runtime type`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\" width=\"50%\">"
      ],
      "metadata": {
        "id": "5OhJSHRyekr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Hardware Accelerator > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\" width=\"50%\">"
      ],
      "metadata": {
        "id": "YGiMsyMvfIxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –¥–∏—Å–ø–ª–µ—è #\n",
        "\n",
        "–í–æ –≤—Ä–µ–º—è –∑–∞–ø–∏—Å–∏ –Ω–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ-–ø–æ–≤—Ç–æ—Ä. –î–ª—è —ç—Ç–æ–≥–æ —Å –ø–æ–º–æ—â—å—é colab **–Ω–∞–º –Ω—É–∂–µ–Ω –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π —ç–∫—Ä–∞–Ω, —á—Ç–æ–±—ã –∏–º–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å —Å—Ä–µ–¥—É** (–∏, —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –∫–∞–¥—Ä—ã).\n",
        "\n",
        "–°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –≤ —Å–ª–µ–¥—É—é—â–µ–π —è—á–µ–π–∫–µ –±—É–¥—É—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –±–∏–±–ª–∏–æ—Ç–µ–∫–∏, –∞ —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–Ω –∏ –∑–∞–ø—É—â–µ–Ω –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π —ç–∫—Ä–∞–Ω üñ•"
      ],
      "metadata": {
        "id": "2OAnJpxhfPNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install pyglet==1.5.1"
      ],
      "metadata": {
        "id": "Wl3RWobQfcnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "iJ7nQ-9yfjo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π #\n",
        "–ü–µ—Ä–≤—ã–º —à–∞–≥–æ–º —è–≤–ª—è–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:\n",
        "\n",
        "- `gym`\n",
        "- `gym-games`: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –≤ —Ç—Ä–µ–Ω–∞–∂–µ—Ä–Ω–æ–º –∑–∞–ª–µ, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é Pygame.\n",
        "- `huggingface_hub`: ü§ó —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ –º–µ—Å—Ç–æ, –≥–¥–µ –ª—é–±–æ–π –∂–µ–ª–∞—é—â–∏–π –º–æ–∂–µ—Ç –¥–µ–ª–∏—Ç—å—Å—è –º–æ–¥–µ–ª—è–º–∏ –∏ –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –∏–∑—É—á–∞—Ç—å –∏—Ö. –í –Ω–µ–º –µ—Å—Ç—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–µ—Ä—Å–∏—è–º–∏, –º–µ—Ç—Ä–∏–∫–∏, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –¥—Ä—É–≥–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—Ç –≤–∞–º –ª–µ–≥–∫–æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—Ç—å —Å –¥—Ä—É–≥–∏–º–∏.\n",
        "\n",
        "–í—ã –º–æ–∂–µ—Ç–µ —É–≤–∏–¥–µ—Ç—å –∑–¥–µ—Å—å –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —É—Å–∏–ª–∏—Ç–µ–ª–µ–π üëâ https://huggingface.co/models?other=reinforce\n",
        "\n",
        "–ò –≤—ã –º–æ–∂–µ—Ç–µ –Ω–∞–π—Ç–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è —Å –≥–ª—É–±–æ–∫–∏–º –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∑–¥–µ—Å—å üëâ https://huggingface.co/models?pipeline_tag=reinforcement-learning"
      ],
      "metadata": {
        "id": "nRLE91M5fnLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt"
      ],
      "metadata": {
        "id": "IBOUe16XgJAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ò–º–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –ø–∞–∫–µ—Ç—ã üì¶\n",
        "–í –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ –∏–º–ø–æ—Ä—Ç—É —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫, –º—ã —Ç–∞–∫–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º:\n",
        "\n",
        "- `imageio`: –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–∂–µ—Ç –Ω–∞–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–≤—Ç–æ—Ä –≤–∏–¥–µ–æ\n"
      ],
      "metadata": {
        "id": "ZICM5v8TgNBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Gym\n",
        "import gym\n",
        "import gym_pygame\n",
        "\n",
        "# Hugging Face Hub\n",
        "from huggingface_hub import notebook_login # –í–æ–π—Ç–∏ –≤ —É—á–µ—Ç–Ω—É—é –∑–∞–ø–∏—Å—å Hugging Face, —á—Ç–æ–±—ã –∏–º–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∂–∞—Ç—å –º–æ–¥–µ–ª–∏ –≤ –•–∞–±.\n",
        "import imageio"
      ],
      "metadata": {
        "id": "qlDWDBeGgb95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, –µ—Å—Ç—å –ª–∏ —É –Ω–∞—Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä\n",
        "\n",
        "–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å GPU, –≤—ã –¥–æ–ª–∂–Ω—ã —É–≤–∏–¥–µ—Ç—å `device:cuda0`"
      ],
      "metadata": {
        "id": "7EaZQAhLgmNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "xwedGiHvg5Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "id": "QkBD67EWg5lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–¢–µ–ø–µ—Ä—å –º—ã –≥–æ—Ç–æ–≤—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –Ω–∞—à –∞–ª–≥–æ—Ä–∏—Ç–º —É—Å–∏–ª–µ–Ω–∏—è üî•"
      ],
      "metadata": {
        "id": "mKyhynXCg_hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ü–µ—Ä–≤—ã–π –∞–≥–µ–Ω—Ç: –ò–≥—Ä–∞–µ–º –≤ –ö–∞—Ä—Ç–ø–æ–ª-v1 ü§ñ"
      ],
      "metadata": {
        "id": "I46nFp5FhHgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –°–æ–∑–¥–∞–π—Ç–µ —Å—Ä–µ–¥—É CartPole –∏ –ø–æ–π–º–∏—Ç–µ, –∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç\n",
        "### [–û–∫—Ä—É–∂–∞—é—â–∞—è —Å—Ä–µ–¥–∞ üéÆ ](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n"
      ],
      "metadata": {
        "id": "sf04gwcDhMRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –ü–æ—á–µ–º—É –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–∞–∫—É—é –ø—Ä–æ—Å—Ç—É—é —Å—Ä–µ–¥—É, –∫–∞–∫ CartPole-v1?\n",
        "–ö–∞–∫ –æ–ø–∏—Å–∞–Ω–æ –≤ [–°–æ–≤–µ—Ç–∞—Ö –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö –ø–æ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html),–∫–æ–≥–¥–∞ –≤—ã –≤–Ω–µ–¥—Ä—è–µ—Ç–µ —Å–≤–æ–π –∞–≥–µ–Ω—Ç —Å –Ω—É–ª—è, –≤–∞–º –Ω—É–∂–Ω–æ **—É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ, –∏ –Ω–∞–π—Ç–∏ –æ—à–∏–±–∫–∏ –≤ –ø—Ä–æ—Å—Ç—ã—Ö —Å—Ä–µ–¥–∞—Ö, –ø—Ä–µ–∂–¥–µ —á–µ–º —É–≥–ª—É–±–ª—è—Ç—å—Å—è**. –ü–æ—Å–∫–æ–ª—å–∫—É –Ω–∞—Ö–æ–¥–∏—Ç—å –æ—à–∏–±–∫–∏ –±—É–¥–µ—Ç –Ω–∞–º–Ω–æ–≥–æ –ø—Ä–æ—â–µ –≤ –ø—Ä–æ—Å—Ç—ã—Ö —Å—Ä–µ–¥–∞—Ö.\n",
        "\n",
        "\n",
        "> –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–¥–∞—Ç—å –∫–∞–∫–æ–π-–Ω–∏–±—É–¥—å ‚Äú–ø—Ä–∏–∑–Ω–∞–∫ –∂–∏–∑–Ω–∏‚Äù –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º —Å –∏–≥—Ä—É—à–∫–∞–º–∏\n",
        "\n",
        "\n",
        "> –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é, –∑–∞—Å—Ç–∞–≤–∏–≤ –µ–µ –∑–∞–ø—É—Å–∫–∞—Ç—å—Å—è –≤–æ –≤—Å–µ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö (–≤—ã –º–æ–∂–µ—Ç–µ —Å—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å RL zoo). –û–±—ã—á–Ω–æ –¥–ª—è —ç—Ç–æ–≥–æ —à–∞–≥–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—É.\n",
        "___\n",
        "### –°—Ä–µ–¥–∞ CartPole-v1\n",
        "\n",
        "> –®–µ—Å—Ç –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω —Å –ø–æ–º–æ—â—å—é —à–∞—Ä–Ω–∏—Ä–∞ –±–µ–∑ –ø—Ä–∏–≤–æ–¥–∞ –∫ —Ç–µ–ª–µ–∂–∫–µ, –∫–æ—Ç–æ—Ä–∞—è –¥–≤–∏–∂–µ—Ç—Å—è –ø–æ —Ä–µ–ª—å—Å–∞–º –±–µ–∑ —Ç—Ä–µ–Ω–∏—è. –ú–∞—è—Ç–Ω–∏–∫ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ –Ω–∞ —Ç–µ–ª–µ–∂–∫—É, –∏ —Ü–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã —É—Ä–∞–≤–Ω–æ–≤–µ—Å–∏—Ç—å —à–µ—Å—Ç, –ø—Ä–∏–∫–ª–∞–¥—ã–≤–∞—è —Å–∏–ª—ã –≤ –ª–µ–≤–æ–º –∏ –ø—Ä–∞–≤–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö –∫ —Ç–µ–ª–µ–∂–∫–µ.\n",
        "\n",
        "\n",
        "\n",
        "–ò—Ç–∞–∫, –º—ã –Ω–∞—á–Ω–µ–º —Å CartPole-v1. –¶–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã —Ç–æ–ª–∫–∞—Ç—å —Ç–µ–ª–µ–∂–∫—É –≤–ª–µ–≤–æ –∏–ª–∏ –≤–ø—Ä–∞–≤–æ **—Ç–∞–∫, —á—Ç–æ–±—ã —à–µ—Å—Ç –æ—Å—Ç–∞–≤–∞–ª—Å—è –≤ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏–∏.**\n",
        "\n",
        "–≠–ø–∏–∑–æ–¥ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è, –µ—Å–ª–∏:\n",
        "- –£–≥–æ–ª –Ω–∞–∫–ª–æ–Ω–∞ —à–µ—Å—Ç–∞ –±–æ–ª—å—à–µ, —á–µ–º ¬±12¬∞\n",
        "- –ü–æ–ª–æ–∂–µ–Ω–∏–µ —Ç–µ–ª–µ–∂–∫–∏ –±–æ–ª—å—à–µ, —á–µ–º ¬±2.4\n",
        "- –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —ç–ø–∏–∑–æ–¥–∞ –±–æ–ª—å—à–µ, —á–µ–º 500\n",
        "\n",
        "–ú—ã –ø–æ–ª—É—á–∞–µ–º –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ üí∞ –≤ —Ä–∞–∑–º–µ—Ä–µ +1 –∫–∞–∂–¥—ã–π —Ä–∞–∑, –∫–æ–≥–¥–∞ –ø–æ–ª—é—Å –æ—Å—Ç–∞–µ—Ç—Å—è –≤ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏–∏."
      ],
      "metadata": {
        "id": "JOtcggzZhXIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ env\n",
        "env = gym.make(env_id)\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã –æ—Ü–µ–Ω–∫–∏\n",
        "eval_env = gym.make(env_id)\n",
        "\n",
        "# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n"
      ],
      "metadata": {
        "id": "G98aO9udhIDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"The State Space is: \", s_size)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è"
      ],
      "metadata": {
        "id": "9GiN3hIxjT9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"The Action Space is: \", a_size)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è"
      ],
      "metadata": {
        "id": "MJ8L7Ga8jbqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –î–∞–≤–∞–π—Ç–µ —Å–æ–∑–¥–∞–¥–∏–º Reinforce –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É\n",
        "–≠—Ç–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –¥–≤—É—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è—Ö:\n",
        "- [–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º PyTorch](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)\n",
        "- [Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)\n",
        "- [–£–ª—É—á—à–µ–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é Chris1nexus](https://github.com/huggingface/deep-rl-class/pull/95)\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png\" alt=\"Reinforce\" width=\"50%\"/>"
      ],
      "metadata": {
        "id": "8y1Ti_Azjlkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ò—Ç–∞–∫, –º—ã —Ö–æ—Ç–∏–º:\n",
        "- –î–≤–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö —Å–ª–æ—è (fc1 –∏ fc2).\n",
        "- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ReLU –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ fc1\n",
        "- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Softmax –¥–ª—è –≤—ã–≤–æ–¥–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–æ –¥–µ–π—Å—Ç–≤–∏—è–º"
      ],
      "metadata": {
        "id": "BPA3g6KhkKjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        # –°–æ–∑–¥–∞–π—Ç–µ –¥–≤–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö (fc-fully connected ) —Å–ª–æ—è\n",
        "\n",
        "    def forward(self, x):\n",
        "        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥\n",
        "        # —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç –≤ fc1, –∑–∞—Ç–µ–º –º—ã –ø—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ReLU\n",
        "        # –≤—ã—Ö–æ–¥—ã fc1 –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤ fc2\n",
        "        # –ú—ã –≤—ã–≤–æ–¥–∏–º softmax\n",
        "    \n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        –£—á–∏—Ç—ã–≤–∞—è —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ\n",
        "        \"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = np.argmax(m)\n",
        "        return action.item(), m.log_prob(action)"
      ],
      "metadata": {
        "id": "D7UkIAZFkRwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "QDtFZIxNk6ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "    \n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = np.argmax(m)\n",
        "        return action.item(), m.log_prob(action)"
      ],
      "metadata": {
        "id": "huxRsnJQk7SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–Ø —Å–æ–≤–µ—Ä—à–∞—é –æ—à–∏–±–∫—É, –º–æ–∂–µ—Ç–µ –ª–∏ –≤—ã –¥–æ–≥–∞–¥–∞—Ç—å—Å—è, –≥–¥–µ?\n",
        "\n",
        "- –ß—Ç–æ–±—ã –≤—ã—è—Å–Ω–∏—Ç—å —ç—Ç–æ, –¥–∞–≤–∞–π—Ç–µ —Å–¥–µ–ª–∞–µ–º –ø–∞—Å –≤–ø–µ—Ä–µ–¥:"
      ],
      "metadata": {
        "id": "rpoxOD-xlbrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "debug_policy = Policy(s_size, a_size, 64).to(device)\n",
        "debug_policy.act(env.reset())"
      ],
      "metadata": {
        "id": "LWrVZaQjlfmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- –ó–¥–µ—Å—å –º—ã –≤–∏–¥–∏–º, —á—Ç–æ –æ—à–∏–±–∫–∞ –≥–ª–∞—Å–∏—Ç `ValueError: –∞—Ä–≥—É–º–µ–Ω—Ç value –¥–ª—è log_prob –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–µ–Ω–∑–æ—Ä–æ–º`\n",
        "\n",
        "- –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ `action` –≤ `m.log_prob(action)` –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å, —Ç–µ–Ω–∑–æ—Ä–æ–º **, –Ω–æ —ç—Ç–æ –Ω–µ —Ç–∞–∫.**\n",
        "\n",
        "- –¢—ã –∑–Ω–∞–µ—à—å –ø–æ—á–µ–º—É? –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é act –∏ –ø–æ–ø—ã—Ç–∞–π—Ç–µ—Å—å –ø–æ–Ω—è—Ç—å, –ø–æ—á–µ–º—É –æ–Ω–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç.\n",
        "\n",
        "–°–æ–≤–µ—Ç üí° : –ß—Ç–æ-—Ç–æ –Ω–µ —Ç–∞–∫ –≤ —ç—Ç–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏. –ü–æ–º–Ω–∏—Ç–µ, —á—Ç–æ **–º—ã —Ö–æ—Ç–∏–º –≤—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ –∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–æ –¥–µ–π—Å—Ç–≤–∏—è–º**.\n"
      ],
      "metadata": {
        "id": "dR5MLshFlks4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ"
      ],
      "metadata": {
        "id": "PMKZsE4TmWao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "    \n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)"
      ],
      "metadata": {
        "id": "_pQQIG4Ymaeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ò—Å–ø–æ–ª—å–∑—É—è CartPole, –æ—Ç–ª–∞–∂–∏–≤–∞—Ç—å –±—ã–ª–æ –ø—Ä–æ—â–µ, –ø–æ—Å–∫–æ–ª—å–∫—É **–º—ã –∑–Ω–∞–µ–º, —á—Ç–æ –æ—à–∏–±–∫–∞ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ –Ω–∞—à–µ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏, –∞ –Ω–µ –∏–∑-–∑–∞ –Ω–∞—à–µ–π –ø—Ä–æ—Å—Ç–æ–π —Å—Ä–µ–¥—ã**."
      ],
      "metadata": {
        "id": "gRtv19mVmrtw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- –ü–æ—Å–∫–æ–ª—å–∫—É **–º—ã —Ö–æ—Ç–∏–º –≤—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ –∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–æ –¥–µ–π—Å—Ç–≤–∏—è–º**, –º—ã –Ω–µ –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `action = np.argmax(m)`, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–æ –≤—Å–µ–≥–¥–∞ –±—É–¥–µ—Ç –≤—ã–≤–æ–¥–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é.\n",
        "\n",
        "- –ù–∞–º –Ω—É–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ `action = m.sample()`, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ—Ç–±–∏—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ –∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π P(.|s)"
      ],
      "metadata": {
        "id": "oCatyd8FPzbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –î–∞–≤–∞–π—Ç–µ –ø–æ—Å—Ç—Ä–æ–∏–º –∞–ª–≥–æ—Ä–∏—Ç–º —É—Å–∏–ª–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
        "–≠—Ç–æ –ø—Å–µ–≤–¥–æ–∫–æ–¥ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —É—Å–∏–ª–µ–Ω–∏—è:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png\" alt=\"Policy gradient pseudocode\"  width=\"70%\"/>"
      ],
      "metadata": {
        "id": "o_xrdklvQV1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- –ö–æ–≥–¥–∞ –º—ã –≤—ã—á–∏—Å–ª—è–µ–º –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å Gt (—Å—Ç—Ä–æ–∫–∞ 6), –º—ã –≤–∏–¥–∏–º, —á—Ç–æ –º—ã –≤—ã—á–∏—Å–ª—è–µ–º —Å—É–º–º—É –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π, **–Ω–∞—á–∏–Ω–∞—è —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ t**.\n",
        "\n",
        "- –ø–æ—á–µ–º—É? –ü–æ—Å–∫–æ–ª—å–∫—É –Ω–∞—à–∞ –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–æ–ª–∂–Ω–∞ —Ç–æ–ª—å–∫–æ **–ø–æ–¥–∫—Ä–µ–ø–ª—è—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–π**: —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –Ω–∞–≥—Ä–∞–¥—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—è, –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã (–ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∏ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã –Ω–µ –∏–∑-–∑–∞ –¥–µ–π—Å—Ç–≤–∏—è), **–≤–∞–∂–Ω—ã —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏—Ö–æ–¥—è—Ç –ø–æ—Å–ª–µ –¥–µ–π—Å—Ç–≤–∏—è**.\n",
        "\n",
        "- –ü—Ä–µ–∂–¥–µ —á–µ–º –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ, –≤—ã –¥–æ–ª–∂–Ω—ã –ø—Ä–æ—á–∏—Ç–∞—Ç—å —ç—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª [ \"–Ω–µ –ø–æ–∑–≤–æ–ª—è–π—Ç–µ –ø—Ä–æ—à–ª–æ–º—É –æ—Ç–≤–ª–µ–∫–∞—Ç—å –≤–∞—Å\"](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you) —ç—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—Ç, –ø–æ—á–µ–º—É –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ–ª–∏—Ç–∏–∫–∏ \"–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –ø–µ—Ä–µ—Ö–æ–¥\".\n",
        "\n",
        "–ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ç–µ—Ä–µ—Å–Ω—É—é —Ç–µ—Ö–Ω–∏–∫—É, –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—É—é [Chris1nexus](https://github.com/Chris1nexus) —á—Ç–æ–±—ã **—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã—á–∏—Å–ª—è—Ç—å –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –Ω–∞ –∫–∞–∂–¥–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ**. –í –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö –æ–±—ä—è—Å–Ω—è–ª–∞—Å—å –ø—Ä–æ—Ü–µ–¥—É—Ä–∞. –ù–µ —Å—Ç–µ—Å–Ω—è–π—Ç–µ—Å—å —Ç–∞–∫–∂–µ [–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ PR](https://github.com/huggingface/deep-rl-class/pull/95)\n",
        "–ù–æ –≤ —Ü–µ–ª–æ–º –∏–¥–µ—è —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã **—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã—á–∏—Å–ª—è—Ç—å –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –Ω–∞ –∫–∞–∂–¥–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ**."
      ],
      "metadata": {
        "id": "h0FOX2jQRBpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–í—Ç–æ—Ä–æ–π –≤–æ–ø—Ä–æ—Å, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–¥–∞—Ç—å, –∑–≤—É—á–∏—Ç —Ç–∞–∫: ** –ø–æ—á–µ–º—É –º—ã –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –ø–æ—Ç–µ—Ä–∏ **? –í—ã –≥–æ–≤–æ—Ä–∏–ª–∏ –æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–º –ø–æ–¥—ä–µ–º–µ, –∞ –Ω–µ –æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–º —Å–ø—É—Å–∫–µ?\n",
        "\n",
        "- –ú—ã —Ö–æ—Ç–∏–º –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—à—É —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ $J(\\theta)$ –Ω–æ –≤ PyTorch, –∫–∞–∫ –∏ –≤ Tensorflow, –ª—É—á—à–µ **–º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ü–µ–ª–µ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é.**\n",
        "    - –ò—Ç–∞–∫, –¥–æ–ø—É—Å—Ç–∏–º, –º—ã —Ö–æ—Ç–∏–º —É—Å–∏–ª–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ 3 –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ. –ü–µ—Ä–µ–¥ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–æ–π —ç—Ç–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è P —Ä–∞–≤–Ω–æ 0.25.\n",
        "    - –ò—Ç–∞–∫, –º—ã —Ö–æ—Ç–∏–º –∏–∑–º–µ–Ω–∏—Ç—å $\\theta$ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã $\\pi_\\theta(a_3|s; \\theta) > 0.25$\n",
        "    - –ü–æ—Å–∫–æ–ª—å–∫—É –≤—Å–µ P –¥–æ–ª–∂–Ω—ã —Å—É–º–º–∏—Ä–æ–≤–∞—Ç—å—Å—è –¥–æ 1, —Ç–æ —Ñ—É–Ω–∫—Ü–∏—è max $\\pi_\\theta(a_3|s; \\theta)$ **—Å–≤–µ–¥–µ—Ç –∫ –º–∏–Ω–∏–º—É–º—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥—Ä—É–≥–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π.**\n",
        "    - –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –º—ã –¥–æ–ª–∂–Ω—ã –≤ PyTorch **, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é min $(1 - \\pi_\\theta(a_3|s; \\theta)$).**\n",
        "    - –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ 0 –ø–æ –º–µ—Ä–µ —Ç–æ–≥–æ, –∫–∞–∫ $\\pi_\\theta(a_3|s; \\theta)$  –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ 1.\n",
        "    - –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç —Å—Ç—Ä–µ–º–∏—Ç—Å—è max $\\pi_\\theta(a_3|s; \\theta)$\n"
      ],
      "metadata": {
        "id": "zq47SR3gTelW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
        "    # –ü–æ–º–æ–≥–∏—Ç–µ –Ω–∞–º –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å –±–∞–ª–ª –≤–æ –≤—Ä–µ–º—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    # –°—Ç—Ä–æ–∫–∞ 3 –ø—Å–µ–≤–¥–æ–∫–æ–¥–∞\n",
        "    for i_episode in range(1, n_training_episodes+1):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = # –ó–ê–î–ê–ß–ê: –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç–µ —Å—Ä–µ–¥—É\n",
        "        # –°—Ç—Ä–æ–∫–∞ 4 –ø—Å–µ–≤–¥–æ–∫–æ–¥–∞\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = # –ó–ê–î–ê–ß–ê: –ø–æ–ª—É—á–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = # –ó–ê–î–ê–ß–ê: —Å–¥–µ–ª–∞–π—Ç–µ —à–∞–≥ –≤ —Å—Ä–µ–¥–µ env\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break \n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "        \n",
        "        # –°—Ç—Ä–æ–∫–∞ 6 –ø—Å–µ–≤–¥–æ–∫–æ–¥–∞: –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–π –Ω–∞–≥—Ä–∞–¥—ã\n",
        "        returns = deque(maxlen=max_t) \n",
        "        n_steps = len(rewards) \n",
        "        # –í—ã—á–∏—Å–ª–∏—Ç–µ –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –Ω–∞ –∫–∞–∂–¥–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ\n",
        "        # –∫–∞–∫ —Å—É–º–º—É –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ —Å –≥–∞–º–º–∞-–¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å—é –≤ –º–æ–º–µ–Ω—Ç t(G_t) + –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –≤ –º–æ–º–µ–Ω—Ç t\n",
        "\n",
        "        # –ó–∞ O(N) –≤—Ä–µ–º—è, –≥–¥–µ N - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤\n",
        "        # (—ç—Ç–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–æ—Ö–æ–¥–∞ G_t —Å–ª–µ–¥—É–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é —ç—Ç–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞\n",
        "        # –ø–æ–∫–∞–∑–∞–Ω–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ 44 2-–≥–æ —á–µ—Ä–Ω–æ–≤–∏–∫–∞ Sutton&Barto 2017)\n",
        "        # G_t = r_(t+1) + r_(t+2) + ...\n",
        "\n",
        "        # –£—á–∏—Ç—ã–≤–∞—è —ç—Ç—É —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É, –≤–æ–∑–≤—Ä–∞—Ç—ã –Ω–∞ –∫–∞–∂–¥–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ t –º–æ–≥—É—Ç –±—ã—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω—ã\n",
        "        # –ø—É—Ç–µ–º –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –±—É–¥—É—â–∏—Ö –≤–æ–∑–≤—Ä–∞—Ç–æ–≤ G_(t + 1) –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ –≤–æ–∑–≤—Ä–∞—Ç–∞ G_t\n",
        "        # G_t = r_(t+1) + –≥–∞–º–º–∞*G_(t+1)\n",
        "        # G_(t-1) = r_t + –≥–∞–º–º–∞ * G_t\n",
        "        # (—ç—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–¥—Ö–æ–¥—É –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, —Å –ø–æ–º–æ—â—å—é –∫–æ—Ç–æ—Ä–æ–≥–æ –º—ã –∑–∞–ø–æ–º–∏–Ω–∞–µ–º —Ä–µ—à–µ–Ω–∏—è, —á—Ç–æ–±—ã\n",
        "        # –∏–∑–±–µ–∂–∞—Ç—å –∏—Ö –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è)\n",
        "\n",
        "        # –≠—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ, –ø–æ—Å–∫–æ–ª—å–∫—É –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω–æ–µ –≤—ã—à–µ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ (—Å–º. —Ç–∞–∫–∂–µ —Å—Ç—Ä. 46 2-–≥–æ —á–µ—Ä–Ω–æ–≤–∏–∫–∞ Sutton&Barto 2017)\n",
        "        # G_(t-1) = r_t + –≥–∞–º–º–∞*r_(t+1) + –≥–∞–º–º–∞*–≥–∞–º–º–∞*r_(t+2) + ...\n",
        "\n",
        "\n",
        "        ## –£—á–∏—Ç—ã–≤–∞—è –≤—ã—à–µ—Å–∫–∞–∑–∞–Ω–Ω–æ–µ, –º—ã –≤—ã—á–∏—Å–ª—è–µ–º –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ t –∫–∞–∫:\n",
        "        # –≥–∞–º–º–∞[t] * –≤–æ–∑–≤—Ä–∞—Ç[t] + –Ω–∞–≥—Ä–∞–¥–∞[t]\n",
        "        #\n",
        "        ## –ú—ã –≤—ã—á–∏—Å–ª—è–µ–º —ç—Ç–æ, –Ω–∞—á–∏–Ω–∞—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞ –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞—è –ø–µ—Ä–≤—ã–º, –ø–æ –ø–æ—Ä—è–¥–∫—É\n",
        "        ## –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –≤—ã—à–µ —Ñ–æ—Ä–º—É–ª—É –∏ –∏–∑–±–µ–∂–∞—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–∏—Å—å\n",
        "        ## –±—ã –µ—Å–ª–∏ –±—ã –º—ã –¥–µ–ª–∞–ª–∏ —ç—Ç–æ –æ—Ç –Ω–∞—á–∞–ª–∞ –¥–æ –∫–æ–Ω—Ü–∞.\n",
        "\n",
        "        ## –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –æ—á–µ—Ä–µ–¥—å \"returns\" –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤–æ–∑–≤—Ä–∞—Ç—ã –≤ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º –ø–æ—Ä—è–¥–∫–µ, –æ—Ç t=0 –¥–æ t=n_steps\n",
        "        ## –±–ª–∞–≥–æ–¥–∞—Ä—è —Ñ—É–Ω–∫—Ü–∏–∏ appendleft(), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ–±–∞–≤–ª—è—Ç—å –∫ –ø–æ–∑–∏—Ü–∏–∏ 0 –∑–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –≤—Ä–µ–º—è O(1)\n",
        "        ## –æ–±—ã—á–Ω—ã–π —Å–ø–∏—Å–æ–∫ python –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª –±—ã O (N) –¥–ª—è —ç—Ç–æ–≥–æ.\n",
        "        for t in range(n_steps)[::-1]:\n",
        "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
        "            returns.appendleft(    ) # –ó–ê–î–ê–ß–ê: –∑–∞–≤–µ—Ä—à–∏—Ç–µ –∑–¥–µ—Å—å       \n",
        "       \n",
        "        ## —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–º\n",
        "        eps = np.finfo(np.float32).eps.item()\n",
        "        \n",
        "        ## eps - —ç—Ç–æ –Ω–∞–∏–º–µ–Ω—å—à–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å –ø–ª–∞–≤–∞—é—â–µ–π —Ç–æ—á–∫–æ–π, –∫–æ—Ç–æ—Ä–æ–µ\n",
        "        # –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—é –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —á–∏—Å–ª–æ–≤–æ–π –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "        returns = torch.tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "        \n",
        "        # –°—Ç—Ä–æ–∫–∞ 7:\n",
        "        policy_loss = []\n",
        "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
        "            policy_loss.append(-log_prob * disc_return)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "        \n",
        "        # –°—Ç—Ä–æ–∫–∞ 8: PyTorch –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ \n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if i_episode % print_every == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
        "        \n",
        "    return scores"
      ],
      "metadata": {
        "id": "HHg5zDPkXSqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### –†–µ—à–µ–Ω–∏–µ"
      ],
      "metadata": {
        "id": "HlhATfY5fijm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
        "    # Help us to calculate the score during the training\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    # Line 3 of pseudocode\n",
        "    for i_episode in range(1, n_training_episodes+1):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "        # Line 4 of pseudocode\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = policy.act(state)\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break \n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "        \n",
        "        # Line 6 of pseudocode: calculate the return\n",
        "        returns = deque(maxlen=max_t) \n",
        "        n_steps = len(rewards) \n",
        "        # Compute the discounted returns at each timestep,\n",
        "        # as \n",
        "        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
        "        #\n",
        "        # In O(N) time, where N is the number of time steps\n",
        "        # (this definition of the discounted return G_t follows the definition of this quantity \n",
        "        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n",
        "        # G_t = r_(t+1) + r_(t+2) + ...\n",
        "        \n",
        "        # Given this formulation, the returns at each timestep t can be computed \n",
        "        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n",
        "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
        "        # G_(t-1) = r_t + gamma* G_t\n",
        "        # (this follows a dynamic programming approach, with which we memorize solutions in order \n",
        "        # to avoid computing them multiple times)\n",
        "        \n",
        "        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n",
        "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
        "        \n",
        "        \n",
        "        ## Given the above, we calculate the returns at timestep t as: \n",
        "        #               gamma[t] * return[t] + reward[t]\n",
        "        #\n",
        "        ## We compute this starting from the last timestep to the first, in order\n",
        "        ## to employ the formula presented above and avoid redundant computations that would be needed \n",
        "        ## if we were to do it from first to last.\n",
        "        \n",
        "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
        "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
        "        ## a normal python list would instead require O(N) to do this.\n",
        "        for t in range(n_steps)[::-1]:\n",
        "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
        "            returns.appendleft( gamma*disc_return_t + rewards[t]   )    \n",
        "            \n",
        "        ## standardization of the returns is employed to make training more stable\n",
        "        eps = np.finfo(np.float32).eps.item()\n",
        "        ## eps is the smallest representable float, which is \n",
        "        # added to the standard deviation of the returns to avoid numerical instabilities        \n",
        "        returns = torch.tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "        \n",
        "        # Line 7:\n",
        "        policy_loss = []\n",
        "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
        "            policy_loss.append(-log_prob * disc_return)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "        \n",
        "        # Line 8: PyTorch prefers gradient descent \n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if i_episode % print_every == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
        "        \n",
        "    return scores"
      ],
      "metadata": {
        "id": "-3mvDHT2fjOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –¢—Ä–µ–Ω–∏—Ä—É–π —ç—Ç–æ\n",
        "- –¢–µ–ø–µ—Ä—å –º—ã –≥–æ—Ç–æ–≤—ã –æ–±—É—á–∞—Ç—å –Ω–∞—à–µ–≥–æ –∞–≥–µ–Ω—Ç–∞.\n",
        "- –ù–æ —Å–Ω–∞—á–∞–ª–∞ –º—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é, —Å–æ–¥–µ—Ä–∂–∞—â—É—é –≤—Å–µ –æ–±—É—á–∞—é—â–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã.\n",
        "- –í—ã –º–æ–∂–µ—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ (–∏ –¥–æ–ª–∂–Ω—ã üòâ)"
      ],
      "metadata": {
        "id": "l0jeKqQ7fsol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cartpole_hyperparameters = {\n",
        "    \"h_size\": 16,\n",
        "    \"n_training_episodes\": 1000,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 1000,\n",
        "    \"gamma\": 1.0,\n",
        "    \"lr\": 1e-2,\n",
        "    \"env_id\": env_id,\n",
        "    \"state_space\": s_size,\n",
        "    \"action_space\": a_size,\n",
        "}"
      ],
      "metadata": {
        "id": "4jG-hT9PfyGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –°–æ–∑–¥–∞–π—Ç–µ –ø–æ–ª–∏—Ç–∏–∫—É –∏ —Ä–∞–∑–º–µ—Å—Ç–∏—Ç–µ –µ–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ\n",
        "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
      ],
      "metadata": {
        "id": "UxGlqPtFgA62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = reinforce(cartpole_policy,\n",
        "                   cartpole_optimizer,\n",
        "                   cartpole_hyperparameters[\"n_training_episodes\"], \n",
        "                   cartpole_hyperparameters[\"max_t\"],\n",
        "                   cartpole_hyperparameters[\"gamma\"], \n",
        "                   100)"
      ],
      "metadata": {
        "id": "KMczmitrgOiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ üìù\n",
        "- –ó–¥–µ—Å—å –º—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Å–æ–±–∏—Ä–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞—à–µ–≥–æ Reinforce –∞–≥–µ–Ω—Ç–∞."
      ],
      "metadata": {
        "id": "RV8ZvxWygXmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
        "  \"\"\"\n",
        "  –û—Ü–µ–Ω–∏—Ç–µ –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —ç–ø–∏–∑–æ–¥–æ–≤ `n_eval_episodes` –∏ –≤–µ—Ä–Ω–∏—Ç–µ —Å—Ä–µ–¥–Ω–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∏ std –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ.\n",
        "  :param env: –°—Ä–µ–¥–∞ –æ—Ü–µ–Ω–∫–∏\n",
        "  :param n_eval_episodes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–∞\n",
        "  :param policy: Reinforce –∞–≥–µ–Ω—Ç\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in range(n_eval_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "      action, _ = policy.act(state)\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "        \n",
        "      if done:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ],
      "metadata": {
        "id": "Hl9tnGOtgiTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –û—Ü–µ–Ω–∏—Ç–µ –Ω–∞—à–µ–≥–æ –∞–≥–µ–Ω—Ç–∞ üìà"
      ],
      "metadata": {
        "id": "5JIe6qyChQ6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_agent(eval_env, \n",
        "               cartpole_hyperparameters[\"max_t\"], \n",
        "               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n",
        "               cartpole_policy)"
      ],
      "metadata": {
        "id": "of4KzBUhhRuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –û–ø—É–±–ª–∏–∫—É–π—Ç–µ –Ω–∞—à—É –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ —Ö–∞–±–µ üî•\n",
        "–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã —É–≤–∏–¥–µ–ª–∏, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è, –º—ã –º–æ–∂–µ–º –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –Ω–∞—à—É –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ —Ö–∞–±–µ ü§ó —Å –ø–æ–º–æ—â—å—é –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –∫–æ–¥–∞.\n",
        "\n",
        "–í–æ—Ç –ø—Ä–∏–º–µ—Ä –∫–∞—Ä—Ç–æ—á–∫–∏ –º–æ–¥–µ–ª–∏:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/modelcard.png\"/>"
      ],
      "metadata": {
        "id": "c9T3JaPXhW3N"
      }
    }
  ]
}