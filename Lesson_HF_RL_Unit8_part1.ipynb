{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhEZv+qpfjyFRFNDHcvHk4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b417f38c783346ffa1b004739fee4faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e2ae823d9fa4ec0945d20aadf933720",
              "IPY_MODEL_52c044dfed79448089f552c8825b9e88",
              "IPY_MODEL_592bd22f687245fdb66695fa65f0e661",
              "IPY_MODEL_4a8f57d8655f40fbb8a7227657635f9a",
              "IPY_MODEL_a864ba8957a8455a904b4b280666e848"
            ],
            "layout": "IPY_MODEL_9cec4547c9b6433cbb8b72150d2a6233"
          }
        },
        "2e2ae823d9fa4ec0945d20aadf933720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_120e12eb336f4846a0d652a3db9adebb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d98bc25510894ba689341ba26313abac",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "52c044dfed79448089f552c8825b9e88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_71cf8b92a5294aed898df778b724625f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_816d09a7396849f5b5694e75a2bf40d4",
            "value": ""
          }
        },
        "592bd22f687245fdb66695fa65f0e661": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_041311015172445eab4a0f78501fe031",
            "style": "IPY_MODEL_37b8092527944fe4b0412ec1d2cb2dd9",
            "value": true
          }
        },
        "4a8f57d8655f40fbb8a7227657635f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_61fa2893b28d44e0b2662733b09f7f90",
            "style": "IPY_MODEL_3b919b567fd54e5fb895674c1f3da361",
            "tooltip": ""
          }
        },
        "a864ba8957a8455a904b4b280666e848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1f1a5dc14174c688f8d030f50429f6a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cc30a7c15c0d4cd0ac2e1dcd65595107",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "9cec4547c9b6433cbb8b72150d2a6233": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "120e12eb336f4846a0d652a3db9adebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d98bc25510894ba689341ba26313abac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71cf8b92a5294aed898df778b724625f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "816d09a7396849f5b5694e75a2bf40d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "041311015172445eab4a0f78501fe031": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37b8092527944fe4b0412ec1d2cb2dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61fa2893b28d44e0b2662733b09f7f90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b919b567fd54e5fb895674c1f3da361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d1f1a5dc14174c688f8d030f50429f6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc30a7c15c0d4cd0ac2e1dcd65595107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Art-phys/Lesson_HF_LR/blob/main/Lesson_HF_RL_Unit8_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ë–ª–æ–∫ 8: –ü—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω—ã–π (–ë–ª–∏–∂–∞–π—à–∏–π) –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ–ª–∏—Ç–∏–∫–∏ (PPO) —Å –ø–æ–º–æ—â—å—é PyTorch ü§ñ\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/thumbnail.png\" alt=\"Unit 8\" width=\"70%\"/>\n",
        "\n",
        "\n",
        "–í —ç—Ç–æ–º –±–ª–æ–∫–Ω–æ—Ç–µ –≤—ã –Ω–∞—É—á–∏—Ç–µ—Å—å **–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–µ–≥–æ –∞–≥–µ–Ω—Ç–∞ PPO —Å –Ω—É–ª—è —Å –ø–æ–º–æ—â—å—é PyTorch, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é CleanRL –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏**.\n",
        "\n",
        "–ß—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –µ–≥–æ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å, –º—ã —Å–æ–±–∏—Ä–∞–µ–º—Å—è –æ–±—É—á–∏—Ç—å –µ–≥–æ –≤ —Å—Ä–µ–¥–µ:\n",
        "\n",
        "- [LunarLander-v2 üöÄ](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n"
      ],
      "metadata": {
        "id": "PHQY2BXNNxpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚¨áÔ∏è –í–æ—Ç –ø—Ä–∏–º–µ—Ä —Ç–æ–≥–æ, —á–µ–≥–æ –≤—ã –¥–æ—Å—Ç–∏–≥–Ω–µ—Ç–µ. ‚¨áÔ∏è"
      ],
      "metadata": {
        "id": "pc4GDEqgPHEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "c71AjaVIPYDx",
        "outputId": "cdf92600-e881-47c6-ccf1-76048713c637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –¶–µ–ª–∏ —ç—Ç–æ–≥–æ –±–ª–æ–∫–Ω–æ—Ç–∞ üèÜ\n",
        "\n",
        "–í –∫–æ–Ω—Ü–µ –∑–∞–ø–∏—Å–Ω–æ–π –∫–Ω–∏–∂–∫–∏ –≤—ã:\n",
        "\n",
        "- –°–º–æ–∂–µ—Ç–µ **–∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–µ–≥–æ –∞–≥–µ–Ω—Ç–∞ PPO —Å –Ω—É–ª—è —Å –ø–æ–º–æ—â—å—é PyTorch**.\n",
        "- –°–º–æ–∂–µ—Ç–µ **–æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–≤–æ–µ–≥–æ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –∏ –∫–æ–¥ –≤ —Ü–µ–Ω—Ç—Ä** —Å —Ö–æ—Ä–æ—à–∏–º –≤–∏–¥–µ–æ–ø–æ–≤—Ç–æ—Ä–æ–º –∏ –æ—Ü–µ–Ω–æ—á–Ω—ã–º –±–∞–ª–ª–æ–º üî•."
      ],
      "metadata": {
        "id": "gvuQR8VVPe0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –≠—Ç–æ—Ç –±–ª–æ–∫–Ω–æ—Ç –≤–∑—è—Ç –∏–∑ –∫—É—Ä—Å–∞ –æ–±—É—á–µ–Ω–∏—è —Å –≥–ª—É–±–æ–∫–∏–º –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"  width=\"70%\"/>"
      ],
      "metadata": {
        "id": "67m9cUALQ6Ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è üèóÔ∏è\n",
        "–ü—Ä–µ–∂–¥–µ —á–µ–º –ø–æ–≥—Ä—É–∑–∏—Ç—å—Å—è –≤  –±–ª–æ–∫–Ω–æ—Ç, –≤–∞–º –Ω—É–∂–Ω–æ:\n",
        "\n",
        "üî≤  üìö –ò–∑—É—á–∏—Ç—å [PPO, –ø—Ä–æ—á–∏—Ç–∞–≤ –±–ª–æ–∫ 8](https://huggingface.co/deep-rl-course/unit8/introduction) ü§ó  "
      ],
      "metadata": {
        "id": "kWGoh9kyRYHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –°–æ–∑–¥–∞–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –¥–∏—Å–ø–ª–µ–π üîΩ\n",
        "\n",
        "–í–æ –≤—Ä–µ–º—è –∑–∞–ø–∏—Å–∏ –Ω–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ-–ø–æ–≤—Ç–æ—Ä. –ß—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ —Å –ø–æ–º–æ—â—å—é colab, **–Ω–∞–º –Ω—É–∂–Ω–æ –∏–º–µ—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π —ç–∫—Ä–∞–Ω, —á—Ç–æ–±—ã –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å —Å—Ä–µ–¥—É** (–∏, —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –∫–∞–¥—Ä—ã).\n",
        "\n",
        "–°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –≤ —Å–ª–µ–¥—É—é—â–µ–π —è—á–µ–π–∫–µ –±—É–¥—É—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –±–∏–±–ª–∏–æ—Ç–µ–∫–∏, –∞ —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–Ω –∏ –∑–∞–ø—É—â–µ–Ω –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π —ç–∫—Ä–∞–Ω üñ•"
      ],
      "metadata": {
        "id": "SJDLDvnmRnuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip install pyglet==1.5\n",
        "!pip3 install pyvirtualdisplay"
      ],
      "metadata": {
        "id": "Y1gbrjx8SIQB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "2VBVkrzNSKra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b03cc3d3-a32e-4794-cdd7-9f64b2ab2923"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f98303ac790>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –£—Å—Ç–∞–Ω–∞–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫ üîΩ\n",
        "–î–ª—è —ç—Ç–æ–≥–æ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º `gym==0.21`"
      ],
      "metadata": {
        "id": "kuUJoyhB6CPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.21\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install huggingface_hub\n",
        "!pip install box2d"
      ],
      "metadata": {
        "id": "owafkNkZ6ULW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "111111111"
      ],
      "metadata": {
        "id": "_CNcvjZauwZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "te8AyPOZQcZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "id": "rLPO3srSIppL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym --upgrade"
      ],
      "metadata": {
        "id": "TrlvQm62EQ8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install box2d box2d-kengz"
      ],
      "metadata": {
        "id": "v868Th31u5c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make(\"LunarLander-v2\")"
      ],
      "metadata": {
        "id": "g8YS3URQ6fFr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "111111111111"
      ],
      "metadata": {
        "id": "8o-jM9Mnux2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –î–∞–≤–∞–π—Ç–µ –∑–∞–∫–æ–¥–∏—Ä—É–µ–º PPO —Å –Ω—É–ª—è —Å –ø–æ–º–æ—â—å—é —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ Costa Huang\n",
        "- –î–ª—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ PPO –º—ã —Å–æ–±–∏—Ä–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ [Costa Huang](https://costa.sh/).\n",
        "- –í –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤—É, —á—Ç–æ–±—ã —É–≥–ª—É–±–∏—Ç—å—Å—è, –≤—ã –º–æ–∂–µ—Ç–µ –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å 37 –¥–µ—Ç–∞–ª—è–º–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —è–¥—Ä–∞: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
        "\n",
        "üëâ –í–∏–¥–µ–æ—É—Ä–æ–∫: https://youtu.be/MEt6rrxH8W4"
      ],
      "metadata": {
        "id": "wWELHm376YZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MEt6rrxH8W4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "metadata": {
        "id": "DGHfaIOh7CMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- –õ—É—á—à–µ –≤—Å–µ–≥–æ —Å–Ω–∞—á–∞–ª–∞ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –≤ —è—á–µ–π–∫–µ –Ω–∏–∂–µ, —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –µ—Å–ª–∏ –≤—ã —É–±—å–µ—Ç–µ –º–∞—à–∏–Ω—É **, –≤—ã –Ω–µ –ø–æ—Ç–µ—Ä—è–µ—Ç–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é**."
      ],
      "metadata": {
        "id": "QJ_sIcZ07PjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Your code here:"
      ],
      "metadata": {
        "id": "xrVRcLD_7qVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –î–æ–±–∞–≤—å—Ç–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å Hugging Face ü§ó\n",
        "- –î–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –Ω–∞—à—É –º–æ–¥–µ–ª—å –≤ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ç–æ—Ä, –Ω–∞–º –Ω—É–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `package_to_hub`"
      ],
      "metadata": {
        "id": "vgHXhyre7vdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- –î–æ–±–∞–≤—å—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –Ω—É–∂–Ω—ã, —á—Ç–æ–±—ã –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –Ω–∞—à—É –º–æ–¥–µ–ª—å –Ω–∞ Hub"
      ],
      "metadata": {
        "id": "CltVtC3T8Aem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, upload_folder\n",
        "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
        "\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import tempfile\n",
        "import json\n",
        "import shutil\n",
        "import imageio\n",
        "\n",
        "from wasabi import Printer\n",
        "msg = Printer()"
      ],
      "metadata": {
        "id": "imgt2riw8RUP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- –î–æ–±–∞–≤—å—Ç–µ –Ω–æ–≤—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç –≤ —Ñ—É–Ω–∫—Ü–∏—é `parse_args()`, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è, –∫—É–¥–∞ –º—ã —Ö–æ—Ç–∏–º –ø–æ–º–µ—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª—å."
      ],
      "metadata": {
        "id": "o-mA-XO58WGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–∞ HuggingFace\n",
        "parser.add_argument(\"--repo-id\", type=str, default=\"Art-phys/ppo-CartPole-v1\", help=\"–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –º–æ–¥–µ–ª–µ–π –∏–∑ Hugging Face Hub {username/repo_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "ffruZ0e_8qgQ",
        "outputId": "49756055-1ff4-4734-9ff7-b8623c3d24c9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-61caea250519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–∞ HuggingFace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--repo-id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Art-phys/ppo-CartPole-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –º–æ–¥–µ–ª–µ–π –∏–∑ Hugging Face Hub {username/repo_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'parser' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- –î–∞–ª–µ–µ –º—ã –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–æ–¥—ã, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –º–æ–¥–µ–ª–∏ –≤ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ç–æ—Ä\n",
        "\n",
        "- –≠—Ç–∏ –º–µ—Ç–æ–¥—ã –ø–æ–∑–≤–æ–ª—è—Ç:\n",
        "  - `_evalutate_agent()`: –æ—Ü–µ–Ω–∏—Ç—å –∞–≥–µ–Ω—Ç–∞.\n",
        "  - `_generate_model_card()`: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ä—Ç–æ—á–∫ –º–æ–¥–µ–ª–∏ –∞–≥–µ–Ω—Ç–∞.\n",
        "  - `_record_video()`: –∑–∞–ø–∏—Å–∞—Ç—å –≤–∏–¥–µ–æ —Å –∞–≥–µ–Ω—Ç–æ–º."
      ],
      "metadata": {
        "id": "VNqZWy529F3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def package_to_hub(repo_id, \n",
        "                model,\n",
        "                hyperparameters,\n",
        "                eval_env,\n",
        "                video_fps=30,\n",
        "                commit_message=\"Push agent to the Hub\",\n",
        "                token= None,\n",
        "                logs=None\n",
        "                ):\n",
        "  \"\"\"\n",
        "  –û—Ü–µ–Ω–∏—Ç–µ, —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ –≤–∏–¥–µ–æ –∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –≤ Hugging Face Hub.\n",
        "–≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä:\n",
        "- –û–Ω –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å\n",
        "- –û–Ω –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–∞—Ä—Ç–æ—á–∫—É –º–æ–¥–µ–ª–∏\n",
        "- –û–Ω –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –≤–∏–¥–µ–æ –∞–≥–µ–Ω—Ç–∞\n",
        "- –û–Ω –ø–µ—Ä–µ–¥–∞–µ—Ç –≤—Å–µ –≤ —Ü–µ–Ω—Ç—Ä\n",
        ":–ø–∞—Ä–∞–º–µ—Ç—Ä repo_id: –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –º–æ–¥–µ–ª–µ–π –∏–∑ —Ü–µ–Ω—Ç—Ä–∞ Hugging Face\n",
        ":–ø–∞—Ä–∞–º–µ—Ç—Ä model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å\n",
        ":–ø–∞—Ä–∞–º–µ—Ç—Ä eval_env: —Å—Ä–µ–¥–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–∞\n",
        ":–ø–∞—Ä–∞–º–µ—Ç—Ä fps: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –≤–∏–¥–µ–æ\n",
        ":–ø–∞—Ä–∞–º–µ—Ç—Ä commit_message: —Å–æ–æ–±—â–µ–Ω–∏–µ –æ —Ñ–∏–∫—Å–∞—Ü–∏–∏\n",
        ":param logs: –∫–∞—Ç–∞–ª–æ–≥ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ –∂—É—Ä–Ω–∞–ª–æ–≤ tensorboard, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã —Ö–æ—Ç–µ–ª–∏ –±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å\n",
        "  \"\"\"\n",
        "  msg.info(\n",
        "      \"–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç, –æ—Ü–µ–Ω–∏—Ç, —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ –≤–∞—à–µ–≥–æ –∞–≥–µ–Ω—Ç–∞,\"\n",
        "      \"—Å–æ–∑–¥–∞–π—Ç–µ –∫–∞—Ä—Ç–æ—á–∫—É –º–æ–¥–µ–ª–∏ –∏ –ø–µ—Ä–µ–Ω–µ—Å–∏—Ç–µ –≤—Å–µ –≤ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ç–æ—Ä.\"\n",
        "      \"–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –¥–æ 1 –º–∏–Ω—É—Ç—ã.\\n \"\n",
        "      \"–≠—Ç–æ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞: –µ—Å–ª–∏ –≤—ã —Å—Ç–æ–ª–∫–Ω–µ—Ç–µ—Å—å —Å –æ—à–∏–±–∫–æ–π, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–∫—Ä–æ–π—Ç–µ –ø—Ä–æ–±–ª–µ–º—É.\"\n",
        "    )\n",
        "  # –®–∞–≥ 1: –ö–ª–æ–Ω–∏—Ä—É–π—Ç–µ –∏–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π\n",
        "  repo_url = HfApi().create_repo(\n",
        "        repo_id=repo_id,\n",
        "        token=token,\n",
        "        private=False,\n",
        "        exist_ok=True,\n",
        "    )\n",
        "  \n",
        "  with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "    tmpdirname = Path(tmpdirname)\n",
        "\n",
        "    # –®–∞–≥ 2: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "    torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n",
        "  \n",
        "    # –®–∞–≥ 3: –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ JSON\n",
        "    mean_reward, std_reward = _evaluate_agent(eval_env, \n",
        "                                           10, \n",
        "                                           model)\n",
        "\n",
        "    # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞—Ç—ã –∏ –≤—Ä–µ–º–µ–Ω–∏\n",
        "    eval_datetime = datetime.datetime.now()\n",
        "    eval_form_datetime = eval_datetime.isoformat()\n",
        "\n",
        "    evaluate_data = {\n",
        "        \"env_id\": hyperparameters.env_id, \n",
        "        \"mean_reward\": mean_reward,\n",
        "        \"std_reward\": std_reward,\n",
        "        \"n_evaluation_episodes\": 10,\n",
        "        \"eval_datetime\": eval_form_datetime,\n",
        "    }\n",
        " \n",
        "    # –ó–∞–ø–∏—Å—å —Ñ–∞–π–ª–∞ JSON\n",
        "    with open(tmpdirname / \"results.json\", \"w\") as outfile:\n",
        "      json.dump(evaluate_data, outfile)\n",
        "\n",
        "    # –®–∞–≥ 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ\n",
        "    video_path =  tmpdirname / \"replay.mp4\"\n",
        "    record_video(eval_env, model, video_path, video_fps)\n",
        "  \n",
        "    # –®–∞–≥ 5: –°–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç–æ—á–∫–∏ –º–æ–¥–µ–ª–∏\n",
        "    generated_model_card, metadata = _generate_model_card(\"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters)\n",
        "    _save_model_card(tmpdirname, generated_model_card, metadata)\n",
        "\n",
        "    # –®–∞–≥ 6: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∂—É—Ä–Ω–∞–ª–∞, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ\n",
        "    if logs:\n",
        "      _add_logdir(tmpdirname, Path(logs))\n",
        "  \n",
        "    msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n",
        "  \n",
        "    repo_url = upload_folder(\n",
        "            repo_id=repo_id,\n",
        "            folder_path=tmpdirname,\n",
        "            path_in_repo=\"\",\n",
        "            commit_message=commit_message,\n",
        "            token=token,\n",
        "        )\n",
        "\n",
        "    msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n",
        "  return repo_url\n",
        "\n",
        "\n",
        "def _evaluate_agent(env, n_eval_episodes, policy):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param policy: The agent\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in range(n_eval_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "    \n",
        "    while done is False:\n",
        "      state = torch.Tensor(state).to(device)\n",
        "      action, _, _, _ = policy.get_action_and_value(state)\n",
        "      new_state, reward, done, info = env.step(action.cpu().numpy())\n",
        "      total_rewards_ep += reward    \n",
        "      if done:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward\n",
        "\n",
        "\n",
        "def record_video(env, policy, out_directory, fps=30):\n",
        "  images = []  \n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  img = env.render(mode='rgb_array')\n",
        "  images.append(img)\n",
        "  while not done:\n",
        "    state = torch.Tensor(state).to(device)\n",
        "    # Take the action (index) that have the maximum expected future reward given that state\n",
        "    action, _, _, _  = policy.get_action_and_value(state)\n",
        "    state, reward, done, info = env.step(action.cpu().numpy()) # We directly put next_state = state for recording logic\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
        "\n",
        "\n",
        "def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n",
        "  \"\"\"\n",
        "  –°–æ–∑–¥–∞–π—Ç–µ –∫–∞—Ä—Ç–æ—á–∫—É –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ç–æ—Ä–∞\n",
        "  :–ø–∞—Ä–∞–º–µ—Ç—Ä model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "  :env_id: –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ä–µ–¥—ã\n",
        "  :mean_reward: —Å—Ä–µ–¥–Ω–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞\n",
        "  :std_reward: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞\n",
        "  :hyperparameters: –æ–±—É—á–∞—é—â–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã\n",
        "  \"\"\"\n",
        "  # –®–∞–≥ 1: –í—ã–±–æ—Ä —Ç–µ–≥–æ–≤\n",
        "  metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n",
        "\n",
        "  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏–º–µ–Ω hyperparams –≤ —Å—Ç—Ä–æ–∫—É\n",
        "  converted_dict = vars(hyperparameters)\n",
        "  converted_str = str(converted_dict)\n",
        "  converted_str = converted_str.split(\", \")\n",
        "  converted_str = '\\n'.join(converted_str)\n",
        " \n",
        "  # –®–∞–≥ 2: –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ –∫–∞—Ä—Ç–æ—á–∫—É –º–æ–¥–µ–ª–∏\n",
        "  model_card = f\"\"\"\n",
        "  # PPO Agent Playing {env_id}\n",
        "\n",
        "  This is a trained model of a PPO agent playing {env_id}.\n",
        "    \n",
        "  # Hyperparameters\n",
        "  ```python\n",
        "  {converted_str}\n",
        "  ```\n",
        "  \"\"\"\n",
        "  return model_card, metadata\n",
        "\n",
        "\n",
        "def generate_metadata(model_name, env_id, mean_reward, std_reward):\n",
        "  \"\"\"\n",
        "  –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —Ç–µ–≥–∏ –¥–ª—è –∫–∞—Ä—Ç–æ—á–∫–∏ –º–æ–¥–µ–ª–∏\n",
        "  :–ø–∞—Ä–∞–º–µ—Ç—Ämodel_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "  :–ø–∞—Ä–∞–º–µ—Ç—Ä env_id: –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ä–µ–¥—ã\n",
        "  :mean_reward: —Å—Ä–µ–¥–Ω–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞\n",
        "  :std_reward: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞\n",
        "  \"\"\"\n",
        "  metadata = {}\n",
        "  metadata[\"tags\"] = [\n",
        "        env_id,\n",
        "        \"ppo\",\n",
        "        \"deep-reinforcement-learning\",\n",
        "        \"reinforcement-learning\",\n",
        "        \"custom-implementation\",\n",
        "        \"deep-rl-course\"\n",
        "  ]\n",
        "\n",
        "  # Add metrics\n",
        "  eval = metadata_eval_result(\n",
        "      model_pretty_name=model_name,\n",
        "      task_pretty_name=\"reinforcement-learning\",\n",
        "      task_id=\"reinforcement-learning\",\n",
        "      metrics_pretty_name=\"mean_reward\",\n",
        "      metrics_id=\"mean_reward\",\n",
        "      metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
        "      dataset_pretty_name=env_id,\n",
        "      dataset_id=env_id,\n",
        "  )\n",
        "\n",
        "  # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π\n",
        "  metadata = {**metadata, **eval}\n",
        "\n",
        "  return metadata\n",
        "\n",
        "\n",
        "def _save_model_card(local_path, generated_model_card, metadata):\n",
        "    \"\"\"–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–∞—Ä—Ç–æ—á–∫—É –º–æ–¥–µ–ª–∏ –¥–ª—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.\n",
        "    :–ø–∞—Ä–∞–º–µ—Ç—Ä local_path: –∫–∞—Ç–∞–ª–æ–≥ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è\n",
        "    :–ø–∞—Ä–∞–º–µ—Ç—Ä generated_model_card: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–∞—Ä—Ç–æ—á–∫–∞ –º–æ–¥–µ–ª–∏,  _generate_model_card()\n",
        "    :–ø–∞—Ä–∞–º–µ—Ç—Ä metadata: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n",
        "    \"\"\"\n",
        "    readme_path = local_path / \"README.md\"\n",
        "    readme = \"\"\n",
        "    if readme_path.exists():\n",
        "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
        "            readme = f.read()\n",
        "    else:\n",
        "        readme = generated_model_card\n",
        "\n",
        "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(readme)\n",
        "\n",
        "    # Save our metrics to Readme metadata\n",
        "    metadata_save(readme_path, metadata)\n",
        "\n",
        "\n",
        "def _add_logdir(local_path: Path, logdir: Path):\n",
        "  \"\"\"–î–æ–±–∞–≤–ª—è–µ—Ç logdir –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π.\n",
        "  :–ø–∞—Ä–∞–º–µ—Ç—Ä local_path: –∫–∞—Ç–∞–ª–æ–≥ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è\n",
        "  :–ø–∞—Ä–∞–º–µ—Ç—Ä logdir: –∫–∞—Ç–∞–ª–æ–≥ logdir\n",
        "  \"\"\"\n",
        "  if logdir.exists() and logdir.is_dir():\n",
        "    # Add the logdir to the repository under new dir called logs\n",
        "    repo_logdir = local_path / \"logs\"\n",
        "    \n",
        "    # Delete current logs if they exist\n",
        "    if repo_logdir.exists():\n",
        "      shutil.rmtree(repo_logdir)\n",
        "\n",
        "    # Copy logdir into repo logdir\n",
        "    shutil.copytree(logdir, repo_logdir)"
      ],
      "metadata": {
        "id": "uX0PZy6v9hmu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- –í –∏—Ç–æ–≥–µ –º—ã –≤—ã–∑—ã–≤–∞–µ–º —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –≤ –∫–æ–Ω—Ü–µ –æ–±—É—á–µ–Ω–∏—è PPO"
      ],
      "metadata": {
        "id": "4YZ6v3iAD3Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã –æ—Ü–µ–Ω–∫–∏\n",
        "eval_env = gym.make(args.env_id)\n",
        "\n",
        "package_to_hub(repo_id = args.repo_id,\n",
        "                model = agent, # –ú–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –º—ã —Ö–æ—Ç–∏–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å\n",
        "                hyperparameters = args,\n",
        "                eval_env = gym.make(args.env_id),\n",
        "                logs= f\"runs/{run_name}\",\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "iFL0Xg5pD-rX",
        "outputId": "9d1b9793-dcf0-4cf2-dfbf-7174f8d2418f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-21f7bdbbf36c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã –æ—Ü–µ–Ω–∫–∏\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m package_to_hub(repo_id = args.repo_id,\n\u001b[1;32m      5\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# –ú–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –º—ã —Ö–æ—Ç–∏–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- –í–æ—Ç –∫–∞–∫ –≤—ã–≥–ª—è–¥—è—Ç ppo.py –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª"
      ],
      "metadata": {
        "id": "fFH3Yl5JEQyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –ø–æ –∞–¥—Ä–µ—Å—É https://docs.cleanrl.dev/rl-algorithms/ppo/#ppopy\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from distutils.util import strtobool\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from huggingface_hub import HfApi, upload_folder\n",
        "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
        "\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import tempfile\n",
        "import json\n",
        "import shutil\n",
        "import imageio\n",
        "\n",
        "from wasabi import Printer\n",
        "msg = Printer()\n",
        "\n",
        "def parse_args():\n",
        "    # fmt: off\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--exp-name\", type=str, default=os.path.basename(__file__).rstrip(\".py\"),\n",
        "        help=\"the name of this experiment\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=1,\n",
        "        help=\"seed of the experiment\")\n",
        "    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n",
        "    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"if toggled, cuda will be enabled by default\")\n",
        "    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
        "        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n",
        "    parser.add_argument(\"--wandb-project-name\", type=str, default=\"cleanRL\",\n",
        "        help=\"the wandb's project name\")\n",
        "    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n",
        "        help=\"the entity (team) of wandb's project\")\n",
        "    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
        "        help=\"weather to capture videos of the agent performances (check out `videos` folder)\")\n",
        "\n",
        "    # Algorithm specific arguments\n",
        "    parser.add_argument(\"--env-id\", type=str, default=\"CartPole-v1\",\n",
        "        help=\"the id of the environment\")\n",
        "    parser.add_argument(\"--total-timesteps\", type=int, default=50000,\n",
        "        help=\"total timesteps of the experiments\")\n",
        "    parser.add_argument(\"--learning-rate\", type=float, default=2.5e-4,\n",
        "        help=\"the learning rate of the optimizer\")\n",
        "    parser.add_argument(\"--num-envs\", type=int, default=4,\n",
        "        help=\"the number of parallel game environments\")\n",
        "    parser.add_argument(\"--num-steps\", type=int, default=128,\n",
        "        help=\"the number of steps to run in each environment per policy rollout\")\n",
        "    parser.add_argument(\"--anneal-lr\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"Toggle learning rate annealing for policy and value networks\")\n",
        "    parser.add_argument(\"--gae\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"Use GAE for advantage computation\")\n",
        "    parser.add_argument(\"--gamma\", type=float, default=0.99,\n",
        "        help=\"the discount factor gamma\")\n",
        "    parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n",
        "        help=\"the lambda for the general advantage estimation\")\n",
        "    parser.add_argument(\"--num-minibatches\", type=int, default=4,\n",
        "        help=\"the number of mini-batches\")\n",
        "    parser.add_argument(\"--update-epochs\", type=int, default=4,\n",
        "        help=\"the K epochs to update the policy\")\n",
        "    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"Toggles advantages normalization\")\n",
        "    parser.add_argument(\"--clip-coef\", type=float, default=0.2,\n",
        "        help=\"the surrogate clipping coefficient\")\n",
        "    parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n",
        "    parser.add_argument(\"--ent-coef\", type=float, default=0.01,\n",
        "        help=\"coefficient of the entropy\")\n",
        "    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n",
        "        help=\"coefficient of the value function\")\n",
        "    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n",
        "        help=\"the maximum norm for the gradient clipping\")\n",
        "    parser.add_argument(\"--target-kl\", type=float, default=None,\n",
        "        help=\"the target KL divergence threshold\")\n",
        "    \n",
        "    # Adding HuggingFace argument\n",
        "    parser.add_argument(\"--repo-id\", type=str, default=\"Art-phys/ppo-CartPole-v1\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    args.batch_size = int(args.num_envs * args.num_steps)\n",
        "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
        "    # fmt: on\n",
        "    return args\n",
        "\n",
        "def package_to_hub(repo_id, \n",
        "                model,\n",
        "                hyperparameters,\n",
        "                eval_env,\n",
        "                video_fps=30,\n",
        "                commit_message=\"Push agent to the Hub\",\n",
        "                token= None,\n",
        "                logs=None\n",
        "                ):\n",
        "  \"\"\"\n",
        "  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
        "  This method does the complete pipeline:\n",
        "  - It evaluates the model\n",
        "  - It generates the model card\n",
        "  - It generates a replay video of the agent\n",
        "  - It pushes everything to the hub\n",
        "  :param repo_id: id of the model repository from the Hugging Face Hub\n",
        "  :param model: trained model\n",
        "  :param eval_env: environment used to evaluate the agent\n",
        "  :param fps: number of fps for rendering the video\n",
        "  :param commit_message: commit message\n",
        "  :param logs: directory on local machine of tensorboard logs you'd like to upload\n",
        "  \"\"\"\n",
        "  msg.info(\n",
        "        \"This function will save, evaluate, generate a video of your agent, \"\n",
        "        \"create a model card and push everything to the hub. \"\n",
        "        \"It might take up to 1min. \\n \"\n",
        "        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n",
        "    )\n",
        "  # Step 1: Clone or create the repo\n",
        "  repo_url = HfApi().create_repo(\n",
        "        repo_id=repo_id,\n",
        "        token=token,\n",
        "        private=False,\n",
        "        exist_ok=True,\n",
        "    )\n",
        "  \n",
        "  with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "    tmpdirname = Path(tmpdirname)\n",
        "\n",
        "    # Step 2: Save the model\n",
        "    torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n",
        "  \n",
        "    # Step 3: Evaluate the model and build JSON\n",
        "    mean_reward, std_reward = _evaluate_agent(eval_env, \n",
        "                                           10, \n",
        "                                           model)\n",
        "\n",
        "    # First get datetime\n",
        "    eval_datetime = datetime.datetime.now()\n",
        "    eval_form_datetime = eval_datetime.isoformat()\n",
        "\n",
        "    evaluate_data = {\n",
        "        \"env_id\": hyperparameters.env_id, \n",
        "        \"mean_reward\": mean_reward,\n",
        "        \"std_reward\": std_reward,\n",
        "        \"n_evaluation_episodes\": 10,\n",
        "        \"eval_datetime\": eval_form_datetime,\n",
        "    }\n",
        " \n",
        "    # Write a JSON file\n",
        "    with open(tmpdirname / \"results.json\", \"w\") as outfile:\n",
        "      json.dump(evaluate_data, outfile)\n",
        "\n",
        "    # Step 4: Generate a video\n",
        "    video_path =  tmpdirname / \"replay.mp4\"\n",
        "    record_video(eval_env, model, video_path, video_fps)\n",
        "  \n",
        "    # Step 5: Generate the model card\n",
        "    generated_model_card, metadata = _generate_model_card(\"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters)\n",
        "    _save_model_card(tmpdirname, generated_model_card, metadata)\n",
        "\n",
        "    # Step 6: Add logs if needed\n",
        "    if logs:\n",
        "      _add_logdir(tmpdirname, Path(logs))\n",
        "  \n",
        "    msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n",
        "  \n",
        "    repo_url = upload_folder(\n",
        "            repo_id=repo_id,\n",
        "            folder_path=tmpdirname,\n",
        "            path_in_repo=\"\",\n",
        "            commit_message=commit_message,\n",
        "            token=token,\n",
        "        )\n",
        "\n",
        "    msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n",
        "  return repo_url\n",
        "\n",
        "def _evaluate_agent(env, n_eval_episodes, policy):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param policy: The agent\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in range(n_eval_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "    \n",
        "    while done is False:\n",
        "      state = torch.Tensor(state).to(device)\n",
        "      action, _, _, _ = policy.get_action_and_value(state)\n",
        "      new_state, reward, done, info = env.step(action.cpu().numpy())\n",
        "      total_rewards_ep += reward    \n",
        "      if done:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward\n",
        "\n",
        "\n",
        "def record_video(env, policy, out_directory, fps=30):\n",
        "  images = []  \n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  img = env.render(mode='rgb_array')\n",
        "  images.append(img)\n",
        "  while not done:\n",
        "    state = torch.Tensor(state).to(device)\n",
        "    # Take the action (index) that have the maximum expected future reward given that state\n",
        "    action, _, _, _  = policy.get_action_and_value(state)\n",
        "    state, reward, done, info = env.step(action.cpu().numpy()) # We directly put next_state = state for recording logic\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
        "\n",
        "\n",
        "def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n",
        "  \"\"\"\n",
        "  Generate the model card for the Hub\n",
        "  :param model_name: name of the model\n",
        "  :env_id: name of the environment\n",
        "  :mean_reward: mean reward of the agent\n",
        "  :std_reward: standard deviation of the mean reward of the agent\n",
        "  :hyperparameters: training arguments\n",
        "  \"\"\"\n",
        "  # Step 1: Select the tags\n",
        "  metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n",
        "\n",
        "  # Transform the hyperparams namespace to string\n",
        "  converted_dict = vars(hyperparameters)\n",
        "  converted_str = str(converted_dict)\n",
        "  converted_str = converted_str.split(\", \")\n",
        "  converted_str = '\\n'.join(converted_str)\n",
        " \n",
        "  # Step 2: Generate the model card\n",
        "  model_card = f\"\"\"\n",
        "  # PPO Agent Playing {env_id}\n",
        "\n",
        "  This is a trained model of a PPO agent playing {env_id}.\n",
        "    \n",
        "  # Hyperparameters\n",
        "  ```python\n",
        "  {converted_str}\n",
        "  ```\n",
        "  \"\"\"\n",
        "  return model_card, metadata\n",
        "\n",
        "def generate_metadata(model_name, env_id, mean_reward, std_reward):\n",
        "  \"\"\"\n",
        "  Define the tags for the model card\n",
        "  :param model_name: name of the model\n",
        "  :param env_id: name of the environment\n",
        "  :mean_reward: mean reward of the agent\n",
        "  :std_reward: standard deviation of the mean reward of the agent\n",
        "  \"\"\"\n",
        "  metadata = {}\n",
        "  metadata[\"tags\"] = [\n",
        "        env_id,\n",
        "        \"ppo\",\n",
        "        \"deep-reinforcement-learning\",\n",
        "        \"reinforcement-learning\",\n",
        "        \"custom-implementation\",\n",
        "        \"deep-rl-course\"\n",
        "  ]\n",
        "\n",
        "  # Add metrics\n",
        "  eval = metadata_eval_result(\n",
        "      model_pretty_name=model_name,\n",
        "      task_pretty_name=\"reinforcement-learning\",\n",
        "      task_id=\"reinforcement-learning\",\n",
        "      metrics_pretty_name=\"mean_reward\",\n",
        "      metrics_id=\"mean_reward\",\n",
        "      metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
        "      dataset_pretty_name=env_id,\n",
        "      dataset_id=env_id,\n",
        "  )\n",
        "\n",
        "  # Merges both dictionaries\n",
        "  metadata = {**metadata, **eval}\n",
        "\n",
        "  return metadata\n",
        "\n",
        "def _save_model_card(local_path, generated_model_card, metadata):\n",
        "    \"\"\"Saves a model card for the repository.\n",
        "    :param local_path: repository directory\n",
        "    :param generated_model_card: model card generated by _generate_model_card()\n",
        "    :param metadata: metadata\n",
        "    \"\"\"\n",
        "    readme_path = local_path / \"README.md\"\n",
        "    readme = \"\"\n",
        "    if readme_path.exists():\n",
        "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
        "            readme = f.read()\n",
        "    else:\n",
        "        readme = generated_model_card\n",
        "\n",
        "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(readme)\n",
        "\n",
        "    # Save our metrics to Readme metadata\n",
        "    metadata_save(readme_path, metadata)\n",
        "\n",
        "def _add_logdir(local_path: Path, logdir: Path):\n",
        "  \"\"\"Adds a logdir to the repository.\n",
        "  :param local_path: repository directory\n",
        "  :param logdir: logdir directory\n",
        "  \"\"\"\n",
        "  if logdir.exists() and logdir.is_dir():\n",
        "    # Add the logdir to the repository under new dir called logs\n",
        "    repo_logdir = local_path / \"logs\"\n",
        "    \n",
        "    # Delete current logs if they exist\n",
        "    if repo_logdir.exists():\n",
        "      shutil.rmtree(repo_logdir)\n",
        "\n",
        "    # Copy logdir into repo logdir\n",
        "    shutil.copytree(logdir, repo_logdir)\n",
        "\n",
        "def make_env(env_id, seed, idx, capture_video, run_name):\n",
        "    def thunk():\n",
        "        env = gym.make(env_id)\n",
        "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "        if capture_video:\n",
        "            if idx == 0:\n",
        "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
        "        env.seed(seed)\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "        return env\n",
        "\n",
        "    return thunk\n",
        "\n",
        "\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, envs):\n",
        "        super().__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 1), std=1.0),\n",
        "        )\n",
        "        self.actor = nn.Sequential(\n",
        "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n",
        "        )\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        logits = self.actor(x)\n",
        "        probs = Categorical(logits=logits)\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "    if args.track:\n",
        "        import wandb\n",
        "\n",
        "        wandb.init(\n",
        "            project=args.wandb_project_name,\n",
        "            entity=args.wandb_entity,\n",
        "            sync_tensorboard=True,\n",
        "            config=vars(args),\n",
        "            name=run_name,\n",
        "            monitor_gym=True,\n",
        "            save_code=True,\n",
        "        )\n",
        "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
        "    writer.add_text(\n",
        "        \"hyperparameters\",\n",
        "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
        "    )\n",
        "\n",
        "    # TRY NOT TO MODIFY: seeding\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
        "\n",
        "    # env setup\n",
        "    envs = gym.vector.SyncVectorEnv(\n",
        "        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
        "    )\n",
        "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
        "\n",
        "    agent = Agent(envs).to(device)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
        "\n",
        "    # ALGO Logic: Storage setup\n",
        "    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
        "    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
        "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "\n",
        "    # TRY NOT TO MODIFY: start the game\n",
        "    global_step = 0\n",
        "    start_time = time.time()\n",
        "    next_obs = torch.Tensor(envs.reset()).to(device)\n",
        "    next_done = torch.zeros(args.num_envs).to(device)\n",
        "    num_updates = args.total_timesteps // args.batch_size\n",
        "\n",
        "    for update in range(1, num_updates + 1):\n",
        "        # Annealing the rate if instructed to do so.\n",
        "        if args.anneal_lr:\n",
        "            frac = 1.0 - (update - 1.0) / num_updates\n",
        "            lrnow = frac * args.learning_rate\n",
        "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
        "\n",
        "        for step in range(0, args.num_steps):\n",
        "            global_step += 1 * args.num_envs\n",
        "            obs[step] = next_obs\n",
        "            dones[step] = next_done\n",
        "\n",
        "            # ALGO LOGIC: action logic\n",
        "            with torch.no_grad():\n",
        "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
        "                values[step] = value.flatten()\n",
        "            actions[step] = action\n",
        "            logprobs[step] = logprob\n",
        "\n",
        "            # TRY NOT TO MODIFY: execute the game and log data.\n",
        "            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
        "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
        "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
        "\n",
        "            for item in info:\n",
        "                if \"episode\" in item.keys():\n",
        "                    print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n",
        "                    writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n",
        "                    writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n",
        "                    break\n",
        "\n",
        "        # bootstrap value if not done\n",
        "        with torch.no_grad():\n",
        "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
        "            if args.gae:\n",
        "                advantages = torch.zeros_like(rewards).to(device)\n",
        "                lastgaelam = 0\n",
        "                for t in reversed(range(args.num_steps)):\n",
        "                    if t == args.num_steps - 1:\n",
        "                        nextnonterminal = 1.0 - next_done\n",
        "                        nextvalues = next_value\n",
        "                    else:\n",
        "                        nextnonterminal = 1.0 - dones[t + 1]\n",
        "                        nextvalues = values[t + 1]\n",
        "                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
        "                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
        "                returns = advantages + values\n",
        "            else:\n",
        "                returns = torch.zeros_like(rewards).to(device)\n",
        "                for t in reversed(range(args.num_steps)):\n",
        "                    if t == args.num_steps - 1:\n",
        "                        nextnonterminal = 1.0 - next_done\n",
        "                        next_return = next_value\n",
        "                    else:\n",
        "                        nextnonterminal = 1.0 - dones[t + 1]\n",
        "                        next_return = returns[t + 1]\n",
        "                    returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n",
        "                advantages = returns - values\n",
        "\n",
        "        # flatten the batch\n",
        "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
        "        b_logprobs = logprobs.reshape(-1)\n",
        "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
        "        b_advantages = advantages.reshape(-1)\n",
        "        b_returns = returns.reshape(-1)\n",
        "        b_values = values.reshape(-1)\n",
        "\n",
        "        # Optimizing the policy and value network\n",
        "        b_inds = np.arange(args.batch_size)\n",
        "        clipfracs = []\n",
        "        for epoch in range(args.update_epochs):\n",
        "            np.random.shuffle(b_inds)\n",
        "            for start in range(0, args.batch_size, args.minibatch_size):\n",
        "                end = start + args.minibatch_size\n",
        "                mb_inds = b_inds[start:end]\n",
        "\n",
        "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
        "                logratio = newlogprob - b_logprobs[mb_inds]\n",
        "                ratio = logratio.exp()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
        "                    old_approx_kl = (-logratio).mean()\n",
        "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
        "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
        "\n",
        "                mb_advantages = b_advantages[mb_inds]\n",
        "                if args.norm_adv:\n",
        "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
        "\n",
        "                # Policy loss\n",
        "                pg_loss1 = -mb_advantages * ratio\n",
        "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
        "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                newvalue = newvalue.view(-1)\n",
        "                if args.clip_vloss:\n",
        "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
        "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
        "                        newvalue - b_values[mb_inds],\n",
        "                        -args.clip_coef,\n",
        "                        args.clip_coef,\n",
        "                    )\n",
        "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
        "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
        "                    v_loss = 0.5 * v_loss_max.mean()\n",
        "                else:\n",
        "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
        "\n",
        "                entropy_loss = entropy.mean()\n",
        "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "            if args.target_kl is not None:\n",
        "                if approx_kl > args.target_kl:\n",
        "                    break\n",
        "\n",
        "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
        "        var_y = np.var(y_true)\n",
        "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "\n",
        "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
        "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
        "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
        "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
        "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
        "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
        "        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
        "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
        "\n",
        "    envs.close()\n",
        "    writer.close()\n",
        "\n",
        "    # Create the evaluation environment\n",
        "    eval_env = gym.make(args.env_id)\n",
        "\n",
        "    package_to_hub(repo_id = args.repo_id,\n",
        "                model = agent, # The model we want to save\n",
        "                hyperparameters = args,\n",
        "                eval_env = gym.make(args.env_id),\n",
        "                logs= f\"runs/{run_name}\",\n",
        "                )\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "8wALvgzZETt4",
        "outputId": "ad489f43-12ae-43c7-bc01-6b346d12e315"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7c62d55fdd9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m     \u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-7c62d55fdd9f>\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     parser.add_argument(\"--exp-name\", type=str, default=os.path.basename(__file__).rstrip(\".py\"),\n\u001b[0m\u001b[1;32m     34\u001b[0m         help=\"the name of this experiment\")\n\u001b[1;32m     35\u001b[0m     parser.add_argument(\"--seed\", type=int, default=1,\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤ huggingface_hub"
      ],
      "metadata": {
        "id": "hSN0wV8lGCt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "b417f38c783346ffa1b004739fee4faa",
            "2e2ae823d9fa4ec0945d20aadf933720",
            "52c044dfed79448089f552c8825b9e88",
            "592bd22f687245fdb66695fa65f0e661",
            "4a8f57d8655f40fbb8a7227657635f9a",
            "a864ba8957a8455a904b4b280666e848",
            "9cec4547c9b6433cbb8b72150d2a6233",
            "120e12eb336f4846a0d652a3db9adebb",
            "d98bc25510894ba689341ba26313abac",
            "71cf8b92a5294aed898df778b724625f",
            "816d09a7396849f5b5694e75a2bf40d4",
            "041311015172445eab4a0f78501fe031",
            "37b8092527944fe4b0412ec1d2cb2dd9",
            "61fa2893b28d44e0b2662733b09f7f90",
            "3b919b567fd54e5fb895674c1f3da361",
            "d1f1a5dc14174c688f8d030f50429f6a",
            "cc30a7c15c0d4cd0ac2e1dcd65595107"
          ]
        },
        "id": "dYbghqM_GBQB",
        "outputId": "6e69d977-73a1-4e51-ff0c-315614565c9b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid.\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –î–∞–≤–∞–π—Ç–µ –Ω–∞—á–Ω–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É üî•\n",
        "- –¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã —Å –Ω—É–ª—è –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–ª–∏ PPO –∏ –¥–æ–±–∞–≤–∏–ª–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é Hugging Face, –º—ã –≥–æ—Ç–æ–≤—ã –Ω–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ üî•\n",
        "- –í–æ-–ø–µ—Ä–≤—ã—Ö, –≤–∞–º –Ω—É–∂–Ω–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å—å –≤–∞—à –∫–æ–¥ –≤ —Å–æ–∑–¥–∞–Ω–Ω—ã–π –≤–∞–º–∏ —Ñ–∞–π–ª –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º `ppo.py `"
      ],
      "metadata": {
        "id": "KHbCvNiUGZTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/step1.png\" alt=\"PPO\" width=\"50%\"/>\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/step2.png\" alt=\"PPO\" width=\"50%\"/>"
      ],
      "metadata": {
        "id": "KJINg2QfGmgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- –¢–µ–ø–µ—Ä—å –Ω–∞–º –ø—Ä–æ—Å—Ç–æ –Ω—É–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –Ω–∞ python, –∏—Å–ø–æ–ª—å–∑—É—è `python <name-of-python-script>.py` —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ —Å –ø–æ–º–æ—â—å—é `argparse`\n",
        "\n",
        "- –í–∞–º —Å–ª–µ–¥—É–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å –±–æ–ª—å—à–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏–Ω–∞—á–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ –±—É–¥–µ—Ç —Å–≤–µ—Ä—Ö—Å—Ç–∞–±–∏–ª—å–Ω—ã–º."
      ],
      "metadata": {
        "id": "TU5qPL0AG2PI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python ppo.py --env-id=\"LunarLander-v2\" --seed=1 --repo-id=\"YOUR_REPO_ID\" --total-timesteps=50000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppRvnR4QHMjJ",
        "outputId": "0338d38e-616d-4a50-e8bb-f3b5f93f57c7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-17 12:33:09.418746: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-17 12:33:10.451049: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-17 12:33:10.451171: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-17 12:33:10.451194: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ppo.py\", line 413, in <module>\n",
            "    envs = gym.vector.SyncVectorEnv(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gym/vector/sync_vector_env.py\", line 52, in __init__\n",
            "    self.envs = [env_fn() for env_fn in env_fns]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gym/vector/sync_vector_env.py\", line 52, in <listcomp>\n",
            "    self.envs = [env_fn() for env_fn in env_fns]\n",
            "  File \"/content/ppo.py\", line 340, in thunk\n",
            "    env.seed(seed)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gym/core.py\", line 241, in __getattr__\n",
            "    return getattr(self.env, name)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gym/core.py\", line 241, in __getattr__\n",
            "    return getattr(self.env, name)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gym/core.py\", line 241, in __getattr__\n",
            "    return getattr(self.env, name)\n",
            "  [Previous line repeated 1 more time]\n",
            "AttributeError: 'LunarLander' object has no attribute 'seed'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ppo.py --total-timesteps=50000"
      ],
      "metadata": {
        "id": "mYGCXgavmZg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã üèÜ\n",
        "–õ—É—á—à–∏–π —Å–ø–æ—Å–æ–± –Ω–∞—É—á–∏—Ç—å—Å—è **- —ç—Ç–æ –ø—Ä–æ–±–æ–≤–∞—Ç—å —á—Ç–æ-—Ç–æ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ**! –ü–æ—á–µ–º—É –±—ã –Ω–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é —Å—Ä–µ–¥—É?"
      ],
      "metadata": {
        "id": "-xejl4NVK9rY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–£–≤–∏–¥–∏–º—Å—è –≤ –±–ª–æ–∫–µ 8, —á–∞—Å—Ç—å 2, –≥–¥–µ –º—ã –±—É–¥–µ–º –æ–±—É—á–∞—Ç—å –∞–≥–µ–Ω—Ç–æ–≤ –∏–≥—Ä–∞—Ç—å –≤ Doom üî•\n",
        "## Keep learning, stay awesome ü§ó"
      ],
      "metadata": {
        "id": "NVlvcyxnLEAY"
      }
    }
  ]
}